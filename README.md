## LLM DAIGT 📝
OPEN PROJECT FOR VLG ✅ <br>

## Table of Contents

- [Introduction](#introduction)
- [Data](#data)
- [Preprocessing](#preprocessing)
- [Word Embeddings](#word-embeddings)
- [Model Building](#model-building)
- [Results](#results)
- [Conclusion](#conclusion)

## Introduction 💡

This project centers around the identification and classification of AI-generated text, a critical task in combating the proliferation of machine-generated content. The code within this repository showcases the process of building machine learning models to discern and classify text data generated by artificial intelligence systems.

## Data 📊

In the given dataset we were provided with the essays generated by HUMAN and AI.
We have to correctly label the essays whether they are HUMAN or AI generated.
The dataset given to us in this competition is higly skewed towards one label <br>![image](https://github.com/akshatshaw/LLM_DAIGT/assets/121082244/ea336766-8007-4436-9a32-a2da2a7aad83)

For tackling this problem we have to use external dataset, Thankfuly we don't have to do this step on our own, many have uploaded dataset containing balanced labels.
Here is the link of the dataset that I have used -> [Dataset](https://www.kaggle.com/datasets/dsluciano/daigt-one-place-all-data)

## Preprocessing 🫧
For the preprocessing part I have used the SPACY liberary and NLTK for Stemming part as lemmatizing from Spacy take a lot of time to compile.
I have removed the Stopwords, Digits, Punctuations, and urls from the raw text, then I have used the NLTK liberary to perform the Stemming.


## Word Embeddings 📌

Machince learning models unfortunatly can't understand text, so we have to somehow convert the text to numbers in order to train a machince learnining model on it.<br>
For this we have various different methods such as Bag of Words, Tf-Idf, N-grams these methods are based on frequency of the word in the sentence, while word embedding such as Word2Vec from the Gensim library is based on the Deep Learning techniques such as CBOW and Skip-gram which uses nueral networks and create a vector based representaion for every word in the vocabulary.<br><br>
For this project I have used every method and the Word2vec seems to perform better than the other.


## Model Building 🚧

For the model building part, I have observed that the Tree based algorithms performs much better than the linear model in the case of NLP classification also the Naive bayes algorithms also perform much betetr than the linear models.<br>
In the given notebook I have used many such different models and also used methods like cross-validation and GridSearchCv for tuning the Hyperparameters.<br>
Furthur I have a sperate notebook in which I have used a Bi-Directional LSTM for which the the preprocessing step is same but the word embedding technique for the LSTM is implimented in the model building step itself.<br>
But the LSTM one does not perform better in this dataset.


## Results 🎉

My final best score in this competition is 0.713.<br>
This competition has ROC_AUC as the scoring matrices i.e. the Area under the curve of the ROC curve.
Also one notable strange thing happening in this competition is that people have very good CV score but their leaderborad score is not good as CV, and I think that this is not due to the overfitting but it is due to the hidden test dataset which is a very different text compaired to the train data, This is also the reason for the poor performance with the LSTMs.<br>


## Conclusion 🤔

In Conclusion I found that in this competition due to the hidden data simpler methods like Tf-Idf and Word2Vec tend to perform much better as compared to the LSTM based models since much of the details of the writting style of Human and AI is different in train and test dataset. 


## Extras 🙌
I have deployed this project end-to-end on StreamLit using its frontend and it is live online [Click here!](https://gpt-detecter.streamlit.app/) to use the app.
Also here is the [GitHub](https://github.com/akshatshaw/AI_Detector) repository for the same.

